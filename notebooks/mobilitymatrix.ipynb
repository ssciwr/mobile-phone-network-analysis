{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e21b9f-a68b-4371-8673-0f9fd2d3ed7d",
   "metadata": {},
   "source": [
    "# Mobility analysis from mobile phone data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d83d90",
   "metadata": {},
   "source": [
    "The goal of this notebook is to analyse a large amount of raw mobile phone data. The target outcome is a mobility matrix that describes mobility between all antennas in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be691764-0033-4ea4-8e68-eb0763bf2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8c68b-331b-4511-b3bd-09356b575943",
   "metadata": {},
   "source": [
    "Create a spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f32bf80-dde3-45f3-b1ac-6d45e326a3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"32g\")\n",
    "    .appName(\"mobility_RJ\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43cf420-efdd-4341-99a9-70616ade712f",
   "metadata": {},
   "source": [
    "Define the data schema used in the dataset. Our rows are of the form `timestamp, userid, zip1, zip2, lat, lon`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11e62b3-ed90-423f-b927-9fd735f281c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType, LongType\n",
    "\n",
    "schema = (\n",
    "    StructType()\n",
    "    .add(\"time\", LongType(), True)\n",
    "    .add(\"user\", StringType(), True)\n",
    "    .add(\"zip1\", IntegerType(), True)\n",
    "    .add(\"zip2\", IntegerType(), True)\n",
    "    .add(\"lat\", DoubleType(), True)\n",
    "    .add(\"lon\", DoubleType(), True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198445e-b020-4de6-915f-24843cd0ce90",
   "metadata": {},
   "source": [
    "Load the dataset from the csv files located in the `data` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd974b79-0711-4e93-afe1-e1798d0d81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../data\", sep=\"|\", schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32a33a-898e-4351-99ad-0090be28f946",
   "metadata": {},
   "source": [
    "To get a grasp of how good we perform, we print the number of data points once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3364c7e-519a-4799-8dee-c3c723189854",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0582b88-9fee-4d73-8e7a-c0771d27cb6d",
   "metadata": {},
   "source": [
    "As a first preprocessing step, we try to clean up the data by introducing nice consecutive indices for antennas. To do so, we create a mapping of `hash(lat, lon) -> idx` such that the index `idx` is consecutive across the antennas that are present in the data. The mapping is stored as a Python `dict` on the frontend but also distributed back to the cluster for use in further data transformations. The first step could be done once and loaded from disk when you are sure that you all antennas are included. The second step needs to be performed even with the mapping being loaded from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7261e4bc-8930-4a14-b7a7-60e675b5295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "antennas_dict = dict(\n",
    "    data.rdd.map(lambda row: hash((row[\"lat\"], row[\"lon\"])))\n",
    "    .distinct()\n",
    "    .zipWithIndex()\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403ba73-6d7c-4787-a1b3-206414ed67fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "antennas = spark.sparkContext.broadcast(antennas_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4d8de-4901-424a-949a-b5da682325f9",
   "metadata": {},
   "source": [
    "Next, we replace the `lat` and `lon` field in the original RDD with above index and at the same time drop unnecessary data. Note that this RDD is never `collect`ed, which means that the entire evaluation is lazy and will be executed in one sweep with the follow-up data transformations. After this transformation, the rows are of the following form: `userid, (timestamp, antennaid)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8142e1-7013-4470-9e82-8978fbb4e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = data.rdd.map(\n",
    "    lambda row: (\n",
    "        row[\"user\"],\n",
    "        (row[\"time\"], antennas.value[hash((row[\"lat\"], row[\"lon\"]))]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea553-48c5-4cf7-9c4c-fcae78aa3763",
   "metadata": {},
   "source": [
    "The next transformation is the cornerstone of the analysis as it does the tracking of all users in a single dataset sweep. After the grouping operation, we drop the userid as it is not needed anymore. The data then has the form `List[(timestamp, antenna_id)]` with one row per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c5113d-d865-42d1-8974-69d2a6215b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = preprocessed.groupByKey().map(lambda row: row[1].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be172ff-7988-4810-af70-aa1a7498127f",
   "metadata": {},
   "source": [
    "Next, we want to identify only transitions between antennas. The following function extracts transitions for a single user. I am not entirely sure how exactly the sorting should be implemented, maybe it is better to do in spark directly. The code with `zip` is a bit weird, but I think it is the shortest code to do the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6934f-3710-45cb-9a8f-1dee3c6fc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transitions(events):\n",
    "    # Is this the correct sorting criterion?\n",
    "    sorted_events = sorted(events, key=lambda e: e[0])\n",
    "    ret = []\n",
    "    for a, b in zip(sorted_events[:-1], sorted_events[1:]):\n",
    "        ret.append((a[1], b[1]))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac54a52-4233-4181-8a9a-12382ac07558",
   "metadata": {},
   "source": [
    "Next, we find all transitions in the dataset. The rows in our dataset are of the form `antenna1, antenna2` with one row per registered transition. Note that we still have not `collect`ed the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ea320-a7e8-41ed-aae6-8d9bdc584df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = grouped.flatMap(extract_transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1abd516-8da8-4015-9879-11c0257fa4a7",
   "metadata": {},
   "source": [
    "Finally, we count the transitions for all pairs of antennas. The `countByValue` operations does an implicit collect (no idea why):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c76e2-4521-445f-8095-ceed6bd68c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = transitions.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12570a3c-c01c-42af-908c-d46e943f3739",
   "metadata": {},
   "source": [
    "These counts can be fed into a dense `numpy` data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51813f-2460-4f4f-a473-8778ba2ede9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "antenna_map = np.full((len(antennas_dict), len(antennas_dict)), 0)\n",
    "entries = np.array([(i0, i1, v) for (i0, i1), v in counts.items()])\n",
    "antenna_map[entries[:, 0], entries[:, 1]] = entries[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc21916-7ef4-44bb-96f0-783149718567",
   "metadata": {},
   "source": [
    "Finally, we export this map for further use in other scripts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bdf929-3b84-404a-8470-d30e64b8099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"antenna_map.npy\", antenna_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
